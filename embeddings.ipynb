{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KGV9NY7m8t1v"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import normalize \n",
    "from xgboost import XGBRegressor\n",
    "import html\n",
    "import re\n",
    "import scipy.stats as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dirGlZvm9BsP"
   },
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "    string = html.unescape(string)\n",
    "    string = string.replace(\"\\\\n\", \" \")\n",
    "    string = re.sub(r\"@[A-Za-z0-9_s(),!?\\'\\`]+\", \"\", string)\n",
    "    string = re.sub(r\"#\", \"\", string)\n",
    "    string = re.sub(r\"\\*\", \"\", string)\n",
    "    string = re.sub(r\"\\'m\", \" am\", string)\n",
    "    string = re.sub(r\"\\'s\", \"\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" not\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" have\", string)\n",
    "    string = re.sub(r\"\\'re\", \" are\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" will\", string)\n",
    "    string = re.sub(r\"\\'d\", \" would\", string)\n",
    "    string = re.sub(r\",\", \"\", string)\n",
    "    string = re.sub(r\"!\", \"\", string)\n",
    "    string = re.sub(r\"\\?\", \" ?\", string)\n",
    "    return remove_stopwords(string.strip().lower())\n",
    "\n",
    "def remove_stopwords(string):\n",
    "    tokens =[token for token in string.split() if token not in stopwords.words('english')]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mtBTJ0Us9Gu5"
   },
   "outputs": [],
   "source": [
    "class Tweet(object):\n",
    "\n",
    "    def __init__(self, id, text, emotion, score):\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.emotion = emotion\n",
    "        self.score = score\n",
    "def reading(path):\n",
    "\n",
    "    lists = list()\n",
    "    with open(path) as input:\n",
    "        for line in input:\n",
    "            line = line.strip()\n",
    "            array = line.split('\\t')\n",
    "            lists.append(Tweet(array[0], clean(array[1]), array[2], float(array[3])))\n",
    "    return lists        \n",
    "\n",
    "Anger_train = reading('anger-ratings-0to1.train.txt')                                   #edit the path here\n",
    "Anger_test  = reading('anger-ratings-0to1.test.gold.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FVGG2RQR_gia"
   },
   "outputs": [],
   "source": [
    "score_train = list()\n",
    "text_train = list()\n",
    "score_test = list()\n",
    "text_test = list()\n",
    "for tweet in Anger_train:\n",
    "    text_train.append(tweet.text)\n",
    "    score_train.append(float(tweet.score))\n",
    "\n",
    "for tweet in Anger_test:\n",
    "    text_test.append(tweet.text)\n",
    "    score_test.append(float(tweet.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "miaUTSDOVMJS"
   },
   "outputs": [],
   "source": [
    "w2v1=gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz', binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "d8dw1gAnYCzl"
   },
   "outputs": [],
   "source": [
    "w2v2=gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/word2vec_twitter_model.bin', binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U--wJfxD08O0"
   },
   "outputs": [],
   "source": [
    "f = open('/content/glove.twitter.27B.200d.txt','r')\n",
    "glove1 = {}\n",
    "num = 1\n",
    "for line in f:\n",
    "  try:\n",
    "    newline = line.split()\n",
    "    word = newline[0]\n",
    "    embedding = [float(digit) for digit in newline[1:]]\n",
    "    glove1[word] = np.array(embedding)\n",
    "    num += 1\n",
    "  except Exception as e:\n",
    "    pass\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2y9L_r2rrhG0"
   },
   "outputs": [],
   "source": [
    "sentiwordnet = PolynomialFeatures(3)\n",
    "def sentiwordnetscore(tweet):\n",
    "    \n",
    "    score = np.zeros(2)\n",
    "    \n",
    "    for word in tweet.split():\n",
    "        synsets = list(swn.senti_synsets(word))\n",
    "        \n",
    "        if synsets:\n",
    "            score[0] += synsets[0].pos_score()\n",
    "            score[1] += synsets[0].neg_score()\n",
    "            \n",
    "    \n",
    "    return normalize(sentiwordnet.fit_transform(np.array([score]).reshape(1, -1))[0].reshape(1, -1))[0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "V1lPj62Cbvcs"
   },
   "outputs": [],
   "source": [
    "def w2vEmbedding(tweet, model, d):\n",
    "    \n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    vector_list = list()\n",
    "    for token in tokens:\n",
    "      try:\n",
    "            vector_list.append(model[token])\n",
    "      \n",
    "      except Exception as e:\n",
    "            pass\n",
    "\n",
    "    if len(vector_list) == 0:\n",
    "        vector = np.zeros(d).tolist()\n",
    "    else:\n",
    "        vector = sum(vector_list) / float(len(vector_list))\n",
    "\n",
    "    return vector\n",
    "\n",
    "def vectorize(tweets):\n",
    "  frame = list()\n",
    "  tmp_vector = DataFrame(list(map(lambda x: w2vEmbedding(x, w2v1, len(w2v1['word'])), tweets)))\n",
    "  frame.append(tmp_vector)\n",
    "  tmp_vector = DataFrame(list(map(lambda x: w2vEmbedding(x, w2v2, len(w2v2['word'])), tweets)))\n",
    "  frame.append(tmp_vector)\n",
    "  tmp_vector = DataFrame(list(map(lambda x: w2vEmbedding(x, glove1, len(glove1['word'])), tweets)))\n",
    "  frame.append(tmp_vector)\n",
    "  tmp_vector = DataFrame(list(map(lambda x: sentiwordnetscore(x), tweets)))\n",
    "  frame.append(tmp_vector)\n",
    "\n",
    "  vectors = pd.concat(frame, axis=1)\n",
    "\n",
    "  return vectors.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SomNcEktc0dN"
   },
   "outputs": [],
   "source": [
    "text_train = vectorize(text_train)\n",
    "text_test = vectorize(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijTHt_KYhkHB",
    "outputId": "5b059102-2a78-4554-cdd0-c60a6f1f1271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:33:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.58878114, 0.57041347])"
      ]
     },
     "execution_count": 160,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(pred, gold):\n",
    "    if len(pred) == len(gold):\n",
    "\n",
    "        \n",
    "        # return zero correlation if predictions are constant\n",
    "        if np.std(pred)==0 or np.std(gold)==0:\n",
    "            return (0,0,0,0)\n",
    "        \n",
    "        pears_corr=st.pearsonr(pred,gold)[0]                                    \n",
    "        spear_corr=st.spearmanr(pred,gold)[0]   \n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "        return np.array([pears_corr,spear_corr])\n",
    "\n",
    "\n",
    "model = XGBRegressor(max_depth=3, n_estimators=20000)\n",
    "\n",
    "x_train = np.array(text_train)\n",
    "score_train = np.array(score_train)\n",
    "model.fit(x_train, score_train)\n",
    "\n",
    "y_pred = model.predict(text_test)\n",
    "\n",
    "evaluate(y_pred, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMMaBzAhuMy8"
   },
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=embedding_info[1], padding=\"post\", truncating=\"post\", \n",
    "                                 dtype='float64')\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=embedding_info[1], padding=\"post\", truncating=\"post\",\n",
    "                                dtype='float64')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conv_1 = Conv1D(300, conv_kernel, activation='relu', input_shape=x_train.shape[1:])\n",
    "pool_1 = MaxPooling1D()\n",
    "flat_1 = Flatten()\n",
    "dense_1 = Dense(30000, activation='relu')\n",
    "dense_2 = Dense(1, activation='sigmoid')\n",
    "drop_1 = Dropout(rate=0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getModel():\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(conv_1)\n",
    "    model.add(pool_1)\n",
    "    \n",
    "\n",
    "    model.add(flat_1)\n",
    "    \n",
    "    model.add(dense_1)\n",
    "    model.add(drop_1)\n",
    "    model.add(dense_2)\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=\"sgd\")\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
